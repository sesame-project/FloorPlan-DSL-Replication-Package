Q: What were the inputs for the navigation stack?

A: The desired robot pose which is expressed in a global reference frame (typically map or odom frame).

Q: What about sensors?

A: The canonical sensor is the laser scanner. This is the standard for navigation, mapping, localization and obstacle avoidance, I've also worked with depth cameras and simulated sonar sensors. Some robots also tilt a laser scanner periodically to construct a pointcloud. The idea is to use the 3D point cloud to construct an octomap that later on is projected to 2D, so detection of obstacles considers 3D obstacles but in the end the navigation is performed in 2D to keep the computation simple.

[[2D laser]]
[[Navigation]]
[[3D laser]]

#P1 